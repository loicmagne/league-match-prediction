{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport wandb\nimport shap\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, make_scorer\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nshap.initjs()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Machine Learning techniques","metadata":{}},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"code","source":"onehotcat = False\ndf = pd.read_csv('../input/league/dataset.csv')\nX = df.drop(columns=['gameId','winner','duration','Unnamed: 0']).copy()\nX = X.drop(columns=[f'ban_100_{k}' for k in range(5)])\nX = X.drop(columns=[f'ban_200_{k}' for k in range(5)])\nY = df[['winner']].copy().values\ndel df\n\n# Remove nans\nimputer_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer_cst = SimpleImputer(missing_values=np.nan, strategy='constant')\n\nnumeric_columns = X.select_dtypes(include=[np.number])\nother_columns = X.select_dtypes(exclude=[np.number])\n\nnumeric_columns_imputed = imputer_mean.fit_transform(numeric_columns)\nother_columns_imputed = imputer_cst.fit_transform(other_columns)\n\nnumeric_columns = pd.DataFrame(numeric_columns_imputed, columns = numeric_columns.columns)\nother_columns = pd.DataFrame(other_columns_imputed, columns = other_columns.columns)\n\nX = pd.concat([numeric_columns,other_columns],axis=1).copy()\ndel numeric_columns, other_columns, other_columns_imputed, numeric_columns_imputed\n\n# Separate categorical features\ncat_feat = []\nfor summ in range(10):\n    cat_feat.append(f'summoner_{summ}_championId')\n    cat_feat.append(f'summoner_{summ}_teamPosition')\n    cat_feat.append(f'summoner_{summ}_tier')\n    cat_feat.append(f'summoner_{summ}_rank')\n    cat_feat.append(f'summoner_{summ}_primaryStyle')\n    cat_feat.append(f'summoner_{summ}_subStyle')\nn_cat = len(cat_feat)\nX_cat = X[cat_feat]\nfor k in range(10):\n    X = X.drop(columns=[\n        f'summoner_{k}_teamPosition',\n        f'summoner_{k}_puuid',\n        f'summoner_{k}_summonerId',\n        f'summoner_{k}_championId',\n        f'summoner_{k}_summoner1Id',\n        f'summoner_{k}_summoner2Id',\n        f'summoner_{k}_primaryStyle',\n        f'summoner_{k}_subStyle',\n        f'summoner_{k}_tier',\n        f'summoner_{k}_rank',\n        f'summoner_{k}_gold',\n        f'summoner_{k}_kills',\n        f'summoner_{k}_deaths',\n        f'summoner_{k}_assists',\n    ])\n\n# Process categorical features\nfloat_columns = X_cat.select_dtypes(include=[np.float])\nX_cat[float_columns.columns] = float_columns.astype(int)\nX_cat_cols = X_cat.columns\nif onehotcat:\n    onehot = OneHotEncoder(sparse=False)\n    X_cat = onehot.fit_transform(X_cat)\n    X_cat = pd.DataFrame(X_cat)\nelse:\n    ordinal = OrdinalEncoder(dtype=np.int)\n    X_cat = ordinal.fit_transform(X_cat)\n    X_cat = pd.DataFrame(X_cat, columns=X_cat_cols)\n\nX = pd.concat([X_cat,X],axis=1).copy()\nprint(f'One hot encodings: {onehotcat}, Dataset shape: {X.shape}')\nprint(f'Class output balance: {Y.sum()/len(Y)}')\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\ndel X_cat, Y, float_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CatBoost","metadata":{}},{"cell_type":"code","source":"cat_features = list(range(n_cat))\nmodel = CatBoostClassifier(\n    iterations=1000,\n    learning_rate=0.15,\n    eval_metric='Accuracy',\n    use_best_model='True',\n    cat_features=cat_features,\n    loss_function='CrossEntropy',\n    # logging_level='Silent'\n)\nparams = {\n    'depth': [4, 6, 8, 10],\n    'learning_rate': [0.001, 0.01, 0.1, 0.15, 0.4],\n    'loss_function': ['Logloss', 'CrossEntropy'],\n    'l2_leaf_reg': [1, 3, 5, 10, 100],\n}\n\"\"\"\nscorer = make_scorer(accuracy_score)\ngridsearch = GridSearchCV(\n    model, \n    param_grid=params,\n    verbose=2,\n    scoring=scorer,\n    cv=5,\n    refit=True\n)\ngridsearch.fit(X_train, Y_train)\n\n\"\"\"\n\nmodel.fit(\n    X_train, Y_train, \n    eval_set=(X_test, Y_test), \n    verbose=True,\n    use_best_model=True\n)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred_test = model.predict(X_test)\nY_pred_train = model.predict(X_train)\naccuracy_test = accuracy_score(Y_test, Y_pred_test)\naccuracy_train = accuracy_score(Y_train, Y_pred_train)\nprint(f\"Test accuracy: {accuracy_test * 100.0:.2f}%\")\nprint(f\"Train accuracy: {accuracy_train * 100.0:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.get_feature_importance(prettified=True).head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import Pool\nfeature_names = X_test.columns.values.tolist()\nX_test_pool = Pool(data=X_test, cat_features=cat_features, feature_names=feature_names)\nshap_values = model.get_feature_importance(X_test_pool,'ShapValues')\n\nexpected_values = shap_values[0,-1]\nshap_values = shap_values[:,:-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LightGBM","metadata":{}},{"cell_type":"code","source":"model = lgb.LGBMClassifier(\n    max_bin=500,\n    num_leaves=60,\n    learning_rate=0.05,\n    num_iterations=1000\n)\nmodel.fit(\n    X_train, Y_train.ravel(),\n    categorical_feature=cat_features,\n)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred_test = model.predict(X_test)\nY_pred_train = model.predict(X_train)\naccuracy_test = accuracy_score(Y_test, Y_pred_test)\naccuracy_train = accuracy_score(Y_train, Y_pred_train)\nprint(f\"Test accuracy: {accuracy_test * 100.0:.2f}%\")\nprint(f\"Train accuracy: {accuracy_train * 100.0:.2f}%\")\nprint(classification_report(Y_test, Y_pred_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost","metadata":{}},{"cell_type":"code","source":"model = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.14,\n    use_label_encoder=False\n)\nmodel.fit(\n    X_train, \n    Y_train.ravel(),\n    eval_set = [(X_train, Y_train), (X_test, Y_test)],\n    eval_metric=['error'],\n    verbose=True,\n)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred_test = model.predict(X_test)\nY_pred_train = model.predict(X_train)\naccuracy_test = accuracy_score(Y_test, Y_pred_test)\naccuracy_train = accuracy_score(Y_train, Y_pred_train)\nprint(f\"Test accuracy: {accuracy_test * 100.0:.2f}%\")\nprint(f\"Train accuracy: {accuracy_train * 100.0:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explainer = shap.Explainer(model)\nshap_values = explainer(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = 65\nshap.plots.waterfall(shap_values[k])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize the first prediction's explanation with a force plot\nshap.plots.force(expected_values, shap_values[k],feature_names=feature_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.plots.beeswarm(shap_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.plots.bar(shap_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.plots.force(shap_values[3500:4000])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network","metadata":{}},{"cell_type":"code","source":"# Dataset\nclass TorchDataset(torch.utils.data.Dataset):\n    def __init__(self,file='../input/league/dataset.csv'):\n        scaler = StandardScaler()\n        label_features = ['gameId','winner','duration','Unnamed: 0']\n        for summ in range(10):\n            label_features.append(f'summoner_{summ}_gold')\n            label_features.append(f'summoner_{summ}_kills')\n            label_features.append(f'summoner_{summ}_deaths')\n            label_features.append(f'summoner_{summ}_assists')\n        # Get data\n        df = pd.read_csv(file)\n        self.X = df.drop(columns=label_features).copy()\n        self.X = self.X.drop(columns=[f'ban_100_{k}' for k in range(5)])\n        self.X = self.X.drop(columns=[f'ban_200_{k}' for k in range(5)])\n        self.Y = df[['winner']].copy().values.astype(np.double)\n        del df\n\n        # Remove nans\n        imputer_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n        imputer_cst = SimpleImputer(missing_values=np.nan, strategy='constant')\n\n        numeric_columns = self.X.select_dtypes(include=[np.number])\n        other_columns = self.X.select_dtypes(exclude=[np.number])\n\n        numeric_columns_imputed = imputer_mean.fit_transform(numeric_columns)\n        other_columns_imputed = imputer_cst.fit_transform(other_columns)\n\n        numeric_columns = pd.DataFrame(numeric_columns_imputed, columns = numeric_columns.columns)\n        other_columns = pd.DataFrame(other_columns_imputed, columns = other_columns.columns)\n\n        self.X = pd.concat([numeric_columns,other_columns],axis=1).copy()\n        del numeric_columns, other_columns, other_columns_imputed, numeric_columns_imputed\n        # Split data for each players\n        summ_features = [{'cat':[], 'cont':[]} for _ in range(10)]\n        for summ in range(10):\n            summ_features[summ]['cat'].append(f'summoner_{summ}_puuid')\n            summ_features[summ]['cat'].append(f'summoner_{summ}_championId')\n            summ_features[summ]['cat'].append(f'summoner_{summ}_teamPosition')\n            # summ_features[summ]['cat'].append(f'summoner_{summ}_summoner1Id')\n            # summ_features[summ]['cat'].append(f'summoner_{summ}_summoner2Id')\n            summ_features[summ]['cat'].append(f'summoner_{summ}_primaryStyle')\n            summ_features[summ]['cat'].append(f'summoner_{summ}_subStyle')\n            summ_features[summ]['cat'].append(f'summoner_{summ}_tier')\n            summ_features[summ]['cat'].append(f'summoner_{summ}_rank')\n            summ_features[summ]['cont'].append(f'summoner_{summ}_summonerLevel')\n            summ_features[summ]['cont'].append(f'summoner_{summ}_lp')\n            summ_features[summ]['cont'].append(f'summoner_{summ}_wr')\n            summ_features[summ]['cont'].append(f'summoner_{summ}_nb')\n            summ_features[summ]['cont'].append(f'summoner_{summ}_champion_mastery')\n            \n        team_features = [[],[]]\n        for team in [0,1]:\n            for feat in ['lp','mastery','wr']:\n                for op in ['mean','std','median','skew','kurtosis','variance']:\n                    team_features[team].append(f'team_{team+1}_{feat}_{op}')\n        \n        # Team features\n        self.team_1_feat = scaler.fit_transform(self.X[team_features[0]].values)\n        self.team_2_feat = scaler.fit_transform(self.X[team_features[1]].values)\n        \n        # Player features\n        self.players_features_cat = []\n        self.players_features_cont = []\n        for summ in range(10):\n            self.players_features_cat.append(self.X[summ_features[summ]['cat']])\n            self.players_features_cont.append(self.X[summ_features[summ]['cont']].values)\n            \n        # Convert floats in cats to ints \n        for i, cat in enumerate(self.players_features_cat):\n            float_columns = cat.select_dtypes(include=[np.float])\n            self.players_features_cat[i][float_columns.columns] = float_columns.astype(int)\n            \n        # Scale cont + Categorize cat\n        self.categorizer = OrdinalEncoder(dtype=np.int)\n        # self.onehot = OneHotEncoder(sparse=False)\n        self.categorizer.fit(np.concatenate(self.players_features_cat))\n        # self.onehot.fit(np.concatenate(self.players_features_cat).astype(str)[:,1:])\n        for i, cat in enumerate(self.players_features_cat):\n            categorized = self.categorizer.transform(cat)\n            # other_onehotencoded = self.onehot.transform(cat.astype(str)[:,1:])\n            self.players_features_cat[i] = categorized.copy()\n        for i, cont in enumerate(self.players_features_cont):\n            self.players_features_cont[i] = scaler.fit_transform(cont.astype(float))\n    \n    def __len__(self):\n        return self.X.shape[0]\n    \n    def __getitem__(self,idx):\n        output = {\n            'team_1': self.team_1_feat[idx],\n            'team_2': self.team_1_feat[idx],\n            'output': self.Y[idx]\n        }\n        for summ in range(10):\n            output[f'summ_{summ}_cat'] = self.players_features_cat[summ][idx]\n            output[f'summ_{summ}_cont'] = self.players_features_cont[summ][idx]\n        return output\n\ndataset = TorchDataset()\nn = len(dataset)\ndataset = dataset\nn_test = int(0.2*n)\nn_train = n - n_test\ntrain_dataset, test_dataset = torch.utils.data.random_split(\n    dataset, \n    [n_train,n_test]\n)\n\nemb_n_seq = [len(cat) for cat in dataset.categorizer.categories_]\nemb_dim_seq = [50, 10, 2, 2, 2, 2, 2]\ncont_dim = dataset[0][f'summ_{2}_cont'].shape[0]\ncat_dim = dataset[0][f'summ_{2}_cat'].shape[0]\nteam_dim = dataset[0][f'team_1'].shape[0]\nprint(f'Trainset size: {len(train_dataset)}, testset size: {len(test_dataset)}')\nprint(f'Size of emb: {emb_n_seq}')\nprint(f'Embedding dim: {emb_dim_seq}')\nprint(f'Number of continuous features: {cont_dim}, number of cat features: {cat_dim}, number of team features: {team_dim}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Neural Network\nclass BasicBlock(torch.nn.Module):\n    def __init__(self,input_features, output_features):\n        super(BasicBlock, self).__init__()\n        self.nn = torch.nn.Sequential(\n            torch.nn.Linear(input_features,output_features),\n            torch.nn.LayerNorm([output_features]),\n            # torch.nn.BatchNorm1d(output_features),\n            torch.nn.ReLU(),\n            torch.nn.Dropout()\n        )\n    def forward(self,x):\n        return self.nn(x)\n    \nclass SummonerNetwork(torch.nn.Module):\n    def __init__(self,emb_n_seq,emb_dim_seq,cont_dim,cat_dim):\n        super(SummonerNetwork, self).__init__()\n        self.embs = torch.nn.ModuleList([\n            torch.nn.Embedding(emb_n_seq[k],emb_dim_seq[k])\n            for k in range(cat_dim)\n        ])\n        self.n_cat = cat_dim\n        self.n = sum(emb_dim_seq) + cont_dim\n        self.nn = torch.nn.Sequential(\n            BasicBlock(self.n, 128),\n            BasicBlock(128,64),\n            BasicBlock(64,64),\n            torch.nn.Linear(64,16)\n        )\n    def forward(self,x_cat,x_cont):\n        emb = torch.cat([\n            self.embs[k](x_cat[:,k])\n            for k in range(self.n_cat)\n        ],dim=1)\n        x = torch.cat([emb,x_cont],dim=1)\n        return self.nn(x)\n\n        \nclass BaseLine(torch.nn.Module):\n    def __init__(self,emb_n_seq,emb_dim_seq,cont_dim,cat_dim,team_dim,device=torch.device('cpu')):\n        super(BaseLine, self).__init__()\n        self.device = device\n        self.summ_nn = SummonerNetwork(emb_n_seq,emb_dim_seq,cont_dim,cat_dim)\n        self.n = 16*10 + team_dim*2\n        self.nn = torch.nn.Sequential(\n            BasicBlock(self.n,256),\n            BasicBlock(256,124),\n            BasicBlock(124,124),\n            BasicBlock(124,64),\n            torch.nn.Linear(64,1)\n        )\n    def forward(self,x):\n        players_features = torch.cat([\n            self.summ_nn(\n                x[f'summ_{summ}_cat'].long().to(self.device),\n                x[f'summ_{summ}_cont'].float().to(self.device)\n            )\n            for summ in range(10)\n        ], dim=1)\n        # team_2_feat = torch.max(players_features[5:],0,True)[0].squeeze()\n        features = torch.cat([\n            players_features,\n            x['team_1'].to(self.device),\n            x['team_2'].to(self.device)\n        ],dim=1)\n        return self.nn(features)\n    \nclass FCEmbedding(torch.nn.Module):\n    def __init__(self,emb_n_seq,emb_dim_seq,cont_dim,cat_dim,team_dim,device=torch.device('cpu')):\n        super(FCEmbedding, self).__init__()\n        self.device = device\n        self.embs = torch.nn.ModuleList([\n            torch.nn.Embedding(emb_n_seq[k],emb_dim_seq[k])\n            for k in range(cat_dim)\n        ])\n        self.n = 10 * (sum(emb_dim_seq) + cont_dim) + team_dim*2\n        self.n_cat = cat_dim\n        self.nn = torch.nn.Sequential(\n            BasicBlock(self.n,512),\n            BasicBlock(512,256),\n            BasicBlock(256,124),\n            BasicBlock(124,124),\n            BasicBlock(124,64),\n            BasicBlock(64,64),\n            torch.nn.Linear(64,1)\n        )\n    def forward(self,x):\n        for summ in range(10):\n            x[f'summ_{summ}_cat'] = x[f'summ_{summ}_cat'].long().to(self.device)\n            x[f'summ_{summ}_cont'] = x[f'summ_{summ}_cont'].float().to(self.device)\n        x_team_1 = x['team_1'].to(self.device)\n        x_team_2 = x['team_2'].to(self.device)\n        \n        # Get cat embeddings\n        cat_emb = torch.cat([\n            self.embs[k](x[f'summ_{summ}_cat'][:,k])\n            for k in range(self.n_cat)\n            for summ in range(10)\n        ],dim=1)\n        cont_feat = torch.cat([\n            x[f'summ_{summ}_cont']\n            for summ in range(10)\n        ], dim=1)\n        features = torch.cat([\n            cat_emb,\n            cont_feat,\n            x_team_1,\n            x_team_2\n        ],dim=1)\n        return self.nn(features)\n    \n        \ndef test_summ_net():\n    summ = 3\n    x_cat = torch.tensor(dataset[0][f'summ_{summ}_cat']).unsqueeze(0).long()\n    x_cont = torch.tensor(dataset[0][f'summ_{summ}_cont']).unsqueeze(0)\n\n    model = SummonerNetwork(emb_n_seq,emb_dim_seq,cont_dim,cat_dim)\n    print(model(x_cat,x_cont))\n    \ndef test_baseline():\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=4,\n        shuffle=True\n    )\n    model = BaseLine(emb_n_seq,emb_dim_seq,cont_dim,cat_dim,team_dim)\n    model.double()\n    for batch in train_loader:\n        print(model(batch))\n        break\n\ndef test_fcembedding():\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=4,\n        shuffle=True\n    )\n    model = FCEmbedding(emb_n_seq,emb_dim_seq,cont_dim,cat_dim,team_dim)\n    model.double()\n    for batch in train_loader:\n        print(model(batch))\n        break\n    \n# test_summ_net()\ntest_baseline()\ntest_fcembedding()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parameters\nbatch_size = 128\nlearning_rate = 5e-4\nepoch = 500\ncriterion = torch.nn.BCEWithLogitsLoss()\nlog = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Dataset\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True\n)\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    shuffle=True\n)\n\n# Model\nmodel = FCEmbedding(emb_n_seq,emb_dim_seq,cont_dim,cat_dim,team_dim,device)\nmodel.double().to(device)\n\n# Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-3)\n\ndef metrics(pred,target):\n    accuracy = accuracy_score(target, pred)\n    precision = precision_score(target, pred)\n    recall = recall_score(target, pred)\n    f1 = f1_score(target, pred)\n    return accuracy, precision, recall, f1\n\n# Wandb\nif log:\n    wandb.init(project=\"league\")\n    wandb.watch(model)\n\nfor k in tqdm(range(epoch)):\n    train_loss, test_loss, train_acc, test_acc = 0,0,0,0\n    # Train\n    model.train()\n    for batch in train_loader:\n        X = batch\n        Y = batch['output'].to(device)\n        pred = model(X)\n        loss = criterion(pred,Y)\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n        optimizer.step()\n        \n        # Add metrics\n        train_acc += torch.sum((torch.sigmoid(pred)>0.5).detach() == Y).item()\n        train_loss += loss.item()\n    # Test  \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            X = batch\n            Y = batch['output'].to(device)\n            pred = model(X)\n            loss = criterion(pred,Y)\n            \n            # Add metrics\n            test_acc += torch.sum((torch.sigmoid(pred)>0.5).detach() == Y).item()\n            test_loss += loss.item()\n    \n    # Log\n    train_loss /= len(train_dataset)\n    train_acc /= n_train\n    test_loss /= len(test_dataset)\n    test_acc /= n_test\n    if log:\n        wandb.log({\n            \"loss_train\" : train_loss,\n            \"loss_test\" : test_loss,\n            \"acc_train\" : train_acc,\n            \"acc_test\" : test_acc,\n        })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"champs = {266:\"Aatrox\",103:\"Ahri\",84:\"Akali\",166:\"Akshan\",12:\"Alistar\",32:\"Amumu\",34:\"Anivia\",1:\"Annie\",523:\"Aphelios\",22:\"Ashe\",136:\"AurelionSol\",268:\"Azir\",432:\"Bard\",53:\"Blitzcrank\",63:\"Brand\",201:\"Braum\",51:\"Caitlyn\",164:\"Camille\",69:\"Cassiopeia\",31:\"Chogath\",42:\"Corki\",122:\"Darius\",131:\"Diana\",119:\"Draven\",36:\"DrMundo\",245:\"Ekko\",60:\"Elise\",28:\"Evelynn\",81:\"Ezreal\",9:\"Fiddlesticks\",114:\"Fiora\",105:\"Fizz\",3:\"Galio\",41:\"Gangplank\",86:\"Garen\",150:\"Gnar\",79:\"Gragas\",104:\"Graves\",887:\"Gwen\",120:\"Hecarim\",74:\"Heimerdinger\",420:\"Illaoi\",39:\"Irelia\",427:\"Ivern\",40:\"Janna\",59:\"JarvanIV\",24:\"Jax\",126:\"Jayce\",202:\"Jhin\",222:\"Jinx\",145:\"Kaisa\",429:\"Kalista\",43:\"Karma\",30:\"Karthus\",38:\"Kassadin\",55:\"Katarina\",10:\"Kayle\",141:\"Kayn\",85:\"Kennen\",121:\"Khazix\",203:\"Kindred\",240:\"Kled\",96:\"KogMaw\",7:\"Leblanc\",64:\"LeeSin\",89:\"Leona\",876:\"Lillia\",127:\"Lissandra\",236:\"Lucian\",117:\"Lulu\",99:\"Lux\",54:\"Malphite\",90:\"Malzahar\",57:\"Maokai\",11:\"MasterYi\",21:\"MissFortune\",62:\"MonkeyKing\",82:\"Mordekaiser\",25:\"Morgana\",267:\"Nami\",75:\"Nasus\",111:\"Nautilus\",518:\"Neeko\",76:\"Nidalee\",56:\"Nocturne\",20:\"Nunu\",2:\"Olaf\",61:\"Orianna\",516:\"Ornn\",80:\"Pantheon\",78:\"Poppy\",555:\"Pyke\",246:\"Qiyana\",133:\"Quinn\",497:\"Rakan\",33:\"Rammus\",421:\"RekSai\",526:\"Rell\",58:\"Renekton\",107:\"Rengar\",92:\"Riven\",68:\"Rumble\",13:\"Ryze\",360:\"Samira\",113:\"Sejuani\",235:\"Senna\",147:\"Seraphine\",875:\"Sett\",35:\"Shaco\",98:\"Shen\",102:\"Shyvana\",27:\"Singed\",14:\"Sion\",15:\"Sivir\",72:\"Skarner\",37:\"Sona\",16:\"Soraka\",50:\"Swain\",517:\"Sylas\",134:\"Syndra\",223:\"TahmKench\",163:\"Taliyah\",91:\"Talon\",44:\"Taric\",17:\"Teemo\",412:\"Thresh\",18:\"Tristana\",48:\"Trundle\",23:\"Tryndamere\",4:\"TwistedFate\",29:\"Twitch\",77:\"Udyr\",6:\"Urgot\",110:\"Varus\",67:\"Vayne\",45:\"Veigar\",161:\"Velkoz\",711:\"Vex\",254:\"Vi\",234:\"Viego\",112:\"Viktor\",8:\"Vladimir\",106:\"Volibear\",19:\"Warwick\",498:\"Xayah\",101:\"Xerath\",5:\"XinZhao\",157:\"Yasuo\",777:\"Yone\",83:\"Yorick\",350:\"Yuumi\",154:\"Zac\",238:\"Zed\",221:\"Zeri\",115:\"Ziggs\",26:\"Zilean\",142:\"Zoe\",143:\"Zyra\"}\ndef visualize_embedding(model,dataset,cat,name):\n    # Visualize embedding\n    pca = PCA(2)\n    plt.figure(figsize=(8, 8), dpi=80)\n    cats = dataset.categorizer.categories_[cat]\n    print(cats)\n    emb = model.embs[cat].weight.cpu().detach().numpy()\n    emb_2d = pca.fit_transform(emb)\n\n    plt.scatter(emb_2d[:,0],emb_2d[:,1])\n    for i, pt in enumerate(emb_2d):\n        plt.text(pt[0]+0.0001,pt[1]-0.0001, cats[i], fontsize=11)\n        \n    plt.yticks([])\n    plt.xticks([])\n    plt.savefig(name)\n    plt.show()\n\nvisualize_embedding(model,dataset,5,'rank')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TabTransformer","metadata":{}},{"cell_type":"code","source":"!pip install tab-transformer-pytorch\nfrom tab_transformer_pytorch import TabTransformer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TorchDataset(torch.utils.data.Dataset):\n    def __init__(self,file='../input/league/dataset.csv'):\n        # Get input\n        df = pd.read_csv(file)\n        X = df.drop(columns=['gameId','winner','duration','Unnamed: 0']).copy()\n        X = X.drop(columns=[f'ban_100_{k}' for k in range(5)])\n        X = X.drop(columns=[f'ban_200_{k}' for k in range(5)])\n        self.Y = df[['winner']].copy().values\n        del df\n\n        # Remove nans\n        imputer_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n        imputer_cst = SimpleImputer(missing_values=np.nan, strategy='constant')\n\n        numeric_columns = X.select_dtypes(include=[np.number])\n        other_columns = X.select_dtypes(exclude=[np.number])\n\n        numeric_columns_imputed = imputer_mean.fit_transform(numeric_columns)\n        other_columns_imputed = imputer_cst.fit_transform(other_columns)\n\n        numeric_columns = pd.DataFrame(numeric_columns_imputed, columns = numeric_columns.columns)\n        other_columns = pd.DataFrame(other_columns_imputed, columns = other_columns.columns)\n\n        X = pd.concat([numeric_columns,other_columns],axis=1).copy()\n        del numeric_columns, other_columns, other_columns_imputed, numeric_columns_imputed\n\n        # Separate categorical features\n        cat_feat = []\n        for summ in range(10):\n            cat_feat.append(f'summoner_{summ}_championId')\n            cat_feat.append(f'summoner_{summ}_teamPosition')\n            cat_feat.append(f'summoner_{summ}_tier')\n            cat_feat.append(f'summoner_{summ}_rank')\n            cat_feat.append(f'summoner_{summ}_primaryStyle')\n            cat_feat.append(f'summoner_{summ}_subStyle')\n        n_cat = len(cat_feat)\n        X_cat = X[cat_feat]\n        for k in range(10):\n            X = X.drop(columns=[\n                f'summoner_{k}_teamPosition',\n                f'summoner_{k}_puuid',\n                f'summoner_{k}_summonerId',\n                f'summoner_{k}_championId',\n                f'summoner_{k}_summoner1Id',\n                f'summoner_{k}_summoner2Id',\n                f'summoner_{k}_primaryStyle',\n                f'summoner_{k}_subStyle',\n                f'summoner_{k}_tier',\n                f'summoner_{k}_rank',\n                f'summoner_{k}_gold',\n                f'summoner_{k}_kills',\n                f'summoner_{k}_deaths',\n                f'summoner_{k}_assists',\n            ])\n\n        # Process categorical features\n        float_columns = X_cat.select_dtypes(include=[np.float])\n        X_cat[float_columns.columns] = float_columns.astype(int)\n        X_cat_cols = X_cat.columns\n        ordinal = OrdinalEncoder(dtype=np.int)\n        X_cat = ordinal.fit_transform(X_cat)\n        self.X_cat = pd.DataFrame(X_cat, columns=X_cat_cols).copy().values\n        self.X_cont = X.copy().values\n        self.cat_sizes = [len(x) for x in ordinal.categories_]\n        self.cont_size = self.X_cont.shape[1]\n    def __len__(self):\n        return self.Y.shape[0]\n    \n    def __getitem__(self,idx):\n        return {\n            'cont': self.X_cont[idx],\n            'cat': self.X_cat[idx],\n            'output': self.Y[idx]\n        }\n\n# Parameters\nbatch_size = 32\nlearning_rate = 3e-4\nepoch = 250\ncriterion = torch.nn.BCEWithLogitsLoss()\nlog = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Dataset\ndataset = TorchDataset()\nn = len(dataset)\nn_test = int(0.2*n)\nn_train = n - n_test\ntrain_dataset, test_dataset = torch.utils.data.random_split(\n    dataset, \n    [n_train,n_test]\n)\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True\n)\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    shuffle=True\n)\n\n# Model\nmodel = TabTransformer(\n    categories = dataset.cat_sizes,     # tuple containing the number of unique values within each category\n    num_continuous = dataset.cont_size ,# number of continuous values\n    dim = 16,                           # dimension, paper set at 32\n    dim_out = 1,                        # binary prediction, but could be anything\n    depth = 6,                          # depth, paper recommended 6\n    heads = 8,                          # heads, paper recommends 8\n    attn_dropout = 0.6,                 # post-attention dropout\n    ff_dropout = 0.6,                   # feed forward dropout\n    mlp_hidden_mults = (4, 2),          # relative multiples of each hidden dimension of the last mlp to logits\n    mlp_act = torch.nn.ReLU(),          # activation for final mlp, defaults to relu, but could be anything else (selu etc)\n)\nmodel.double()\nmodel.to(device)\n\n# Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n\n# Wandb\nif log:\n    wandb.init(project=\"league\")\n    wandb.watch(model)\n\nfor k in range(epoch):\n    train_loss, test_loss, train_acc, test_acc = 0,0,0,0\n    # Train\n    model.train()\n    for batch in tqdm(train_loader):\n        cat = batch['cat'].to(device)\n        cont = batch['cont'].to(device)\n        Y = batch['output'].to(device)\n        pred = model(cat,cont)\n        loss = criterion(pred,Y.float())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Add metrics\n        train_acc += torch.sum((torch.sigmoid(pred)>0.5).detach() == Y).item()\n        train_loss += loss.item()\n    # Test  \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            cat = batch['cat'].to(device)\n            cont = batch['cont'].to(device)\n            Y = batch['output'].to(device)\n            pred = model(cat,cont)\n            loss = criterion(pred,Y.float())\n            \n            # Add metrics\n            test_acc += torch.sum((torch.sigmoid(pred)>0.5).detach() == Y).item()\n            test_loss += loss.item()\n    \n    # Log\n    train_loss /= len(train_dataset)\n    train_acc /= n_train\n    test_loss /= len(test_dataset)\n    test_acc /= n_test\n    if log:\n        wandb.log({\n            \"loss_train\" : train_loss,\n            \"loss_test\" : test_loss,\n            \"acc_train\" : train_acc,\n            \"acc_test\" : test_acc,\n        })\n    else:\n        print(f'Loss train: {train_loss}, Loss test: {test_loss}, Acc train: {train_acc}, Acc test: {test_acc}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}